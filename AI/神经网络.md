# 神经网络

## 神经网络入门

### 机器学习简介

* 机器学习是将无序数据转化为价值的方法

* 机器学习的应用举例
  * 分类问题——图像识别、垃圾邮件识别
  * 回归问题（产生的是一个数）——股价预测、房价预测
  * 排序问题——点击率预估、推荐
  * 生成问题——图像生成、图像风格转换、图像文字描述生成

* 机器学习应用流程

  ![3-1](E:\markdown笔记\笔记图片\3-1.png)

* 机器学习岗位的职责

  * 数据处理（采集+去燥）
  * 模型训练（特征+模型）
  * 模型评估与优化（MSE、F1-score、AUC+调参)
  * 模型应用（A/B测试）

### 深度学习简介

* 深度学习与机器学习
  * 机器学习是实现人工智能的方法
  * 深度学习是实现机器学习算法的技术
* 深度学习算法集合
  * 卷积神经网络
  * 循环神经网络
  * 自动编码器
  * 稀疏编码
  * 深度信念网络
  * 限制玻尔兹曼机
  * 深度学习+强化学习=深度强化学习
* 深度学习的进展
  * 在图像分类中，NASNet已经将图像分类的错误率降低到了5%以下
  * 机器翻译
  * 图像生成
  * AlphaGo

### 神经网络

#### 1.神经元

* 原理
  ![3-2](E:\markdown笔记\笔记图片\3-2.png)

  如上图所示，其中x为**特征**，W为**权重**，f为**激活函数**，b为**偏置**（在图的右上角我们可以看到一个二维数轴，如果没有 偏置b的话，那么y=x只能过原点活动，加了偏置b的话就可以上下活动了）。下面的数组x表示的就是我们要买一套房子时，我们做出判断需要参考的特征，再结合这些特征就是我们买房子时这些特征在我们心中的权重。

* 举例

  ![3-3](E:\markdown笔记\笔记图片\3-3.png)

  在例子中x为特征，w为权重，H为激活函数。在上图的例子中省略了偏置b，演示了原理中的那个算式

* W和b其实定义的是一个分类线或者是一个分类面

  ![3-4](E:\markdown笔记\笔记图片\3-4.png)

#### 2.逻辑回归模型

* 二分类的逻辑斯蒂回归模型

  ![3-5](E:\markdown笔记\笔记图片\3-5.png)

  其中激活函数sigmoid为f(x)，f(x)的取值范围是0-1之间，概率的取值范围也是0-1之间。所以定义了概率P(Y=0|x)和P(Y=1|x)。

* 多分类的逻辑斯蒂回归模型

  > 就要有多个神经元组成，在下图中有两个神经元。		

  * W从向量扩展为矩阵

  * 输出W*x则变为向量

    ![3-6](E:\markdown笔记\笔记图片\3-6.png)

  * 举例

    ![3-7](E:\markdown笔记\笔记图片\3-7.png)

* 将二分类拓展为多分类的逻辑斯蒂回归模型

  ![3-8](E:\markdown笔记\笔记图片\3-8.png)

  上图中E^(-wx)为激活函数，他与1的归一化其实就是二分类的逻辑斯蒂回归模型。

  ![3-9](E:\markdown笔记\笔记图片\3-9.png)

  将W*x变为多输出，再与1进行归一化就形成了多分类的逻辑回归模型。

  * 例子：两个神经元，其中归一化就是加起来

    ![3-10](E:\markdown笔记\笔记图片\3-10.png)

#### 3.目标函数（损失函数）

* 目标函数——衡量对数据的拟合程度，通常也被称为损失函数

  ![3-11](E:\markdown笔记\笔记图片\3-11.png)

  在二分类中，其中x1是一个向量，y1是类别值（真实值），y1‘是预测值，Loss=0.2是差值，0.2也是目标函数的值。在优化神经网络的时候，我们的目标就是使这个差值变得更小。

  ![3-12](E:\markdown笔记\笔记图片\3-12.png)

  在多分类中，将真实值进行[One-Hot编码](http://www.cnblogs.com/lianyingteng/p/7755545.html)再减去预测值，再求绝对值，最终得到差值。

* 常用目标函数——平方差损失

  ![3-13](E:\markdown笔记\笔记图片\3-13.png)

  先用真实值y减去预测值Model(x)，求平方，再乘1/2。

  如果是多分类的话，需要将y进行One-Hot编码，然后再计算。

* 目标函数——交叉熵[shāng]损失（更适合多分类）

  ![3-14](E:\markdown笔记\笔记图片\3-14.png)

#### 4.神经网络训练

* 梯度下降算法

  > 类似于下山算法，第一步先找到方向，第二步向前走一步；然后再重复第一步。

  ![3-15](E:\markdown笔记\笔记图片\3-15.png)

  其中红色矩形框所对应的就是方向。在初等数学中图中右面所示，一个函数的方向是用他的导数所表示的。

  ![3-16](E:\markdown笔记\笔记图片\3-16.png)

  其中学习率阿尔法是人为事先设定的，他对神经网络算法的影响如图右所示。

### Tensorflow基础

* 计算图模型

  ![3-17](E:\markdown笔记\笔记图片\3-17.png)

  上图介绍了命令式编程是先输入后计算，而计算图模型（声明式编程）则是先定义计算模型，再输入计算。

  ![3-18](E:\markdown笔记\笔记图片\3-18.png)

  Tensorflow就是基于计算图模型设计的，神经网络结构是定义好的，但他的数据输入是用户提供的。

* 命令式编程和声明式编程比较

  ![3-19](E:\markdown笔记\笔记图片\3-19.png)

### 神经网络Tensorflow实战

#### 1.数据集的介绍（data_introduction.ipynb）

* CIFAR-10

数据集下载[CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)

* 注意
  * python3中cPickle模块已经更名为_pickle,所以在python3中导入时可以使用

    `import _pickle as cPickle`

  * 在使用_pickle.load(f)时，解决UnicodeDecodeError: 'ascii' codec can't decode byte 0x8b in position 6: ordinal not in range(128)

    `data = cPickle.load(f,encoding='bytes')`

    参考博客：[解决UnicodeDecodeError: 'ascii' codec can't decode byte 0x8b in position 6: ordinal not in range(128)](https://blog.csdn.net/weixin_39449570/article/details/78622573)

#### 2.单个神经元实现（二分类逻辑斯蒂回归模型实现）（[neuron.ipynb](http://localhost:8888/notebooks/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/02-nn-basic/neuron.ipynb))

* 相关API介绍
  * [tf.placeholderce](https://tensorflow.google.cn/api_docs/python/tf/placeholder)

    为填充的张量(Tensor)插入一个占位符。

  * [tf.get_variable](https://tensorflow.google.cn/api_docs/python/tf/get_variable)

    获取具有这些参数的现有变量或创建一个新变量。

  * [tf.matmul](https://tensorflow.google.cn/api_docs/python/tf/linalg/matmul)

    将矩阵a乘以矩阵b，产生a * b。

  * [tf.nn.sigmoid](https://tensorflow.google.cn/api_docs/python/tf/math/sigmoid)

    Computes sigmoid of `x` element-wise.

    Specifically, `y = 1 / (1 + exp(-x))`.

  * [tf.reshape](https://tensorflow.google.cn/api_docs/python/tf/reshape)

    重塑张量。

  * [tf.cast](https://tensorflow.google.cn/api_docs/python/tf/dtypes/cast)

    将张量转换为新类型。

  * [tf.reduce_mean](https://tensorflow.google.cn/api_docs/python/tf/math/reduce_mean)

    计算张量维度的元素平均值。

  * [tf.square](https://tensorflow.google.cn/api_docs/python/tf/math/square)

    计算x元素的平方。

  * [tf.equal](https://tensorflow.google.cn/api_docs/python/tf/math/equal)

    以元素方式返回（x == y）的真值。

  * [tf.train.AdamOptimizer]()

    实现Adam算法的优化器。

  * [tf.global_variable_initializer]()

  * [tf.Session](https://tensorflow.google.cn/api_docs/python/tf/Session)

    用于运行TensorFlow操作的类。

#### 3.多个神经元实现（多分类的逻辑斯蒂回归模型实现）

* 将所有类别数据添加进来

  ```python
   # 将所有类别数据添加进来
   all_data.append(data)
   all_labels.append(labels)
  ```

* 设置10个神经元

  ```python
  # 参数中的10：有10个累别，我们用了10个神经元
  w = tf.get_variable('w', [x.get_shape()[-1], 10],
                     initializer=tf.random_normal_initializer(0, 1))
  
  # b的shape为(10,)
  # b是偏置，name为b，shape为一维，initializer用常量0来初始化
  b = tf.get_variable('b', [10], initializer=tf.constant_initializer(0.0))
  ```

* 平方差损失函数

  ```python
  # mean square loss 
  # 笔记中的示例：1+e^x
  # tf api：e^x /sum(e^x)
  p_y = tf.nn.softmax(y_)
  # 5 --> [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
  y_one_hot = tf.one_hot(y, 10, dtype=tf.float32)
  # 平方差损失函数
  loss = tf.reduce_mean(tf.square(y_one_hot - p_y)) # 求出差的平方的均值
  ```

* 交叉熵损失函数

  ```python
  # 交叉熵损失函数，该函数做了以下三件事
  # 1.y_ -> sofmax
  # 2.y -> one_hot
  # 3.loss y log y_
  loss = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=y_)
  ```

* 计算预测值

  ```python
  # 最大值的indices
  predict = tf.argmax(y_, 1)
  ```

### 神经网络结构

* 包含一个隐含层的神经网络的计算

  ![3-20](E:\markdown笔记\笔记图片\3-20.png)

* 正向传播：多层神经网络的计算

  ![3-21](E:\markdown笔记\笔记图片\3-21.png)

* 反向传播

  ![3-22](E:\markdown笔记\笔记图片\3-22.png)

  ![3-23](E:\markdown笔记\笔记图片\3-23.png)

  ![3-24](E:\markdown笔记\笔记图片\3-24.png)

  ![3-25](E:\markdown笔记\笔记图片\3-25.png)

* 神经网络训练优化
  ![3-26](E:\markdown笔记\笔记图片\3-26.png)

  ![3-27](E:\markdown笔记\笔记图片\3-27.png)

  ![3-28](E:\markdown笔记\笔记图片\3-28.png)

  ![3-29](E:\markdown笔记\笔记图片\3-29.png)

  ![3-30](E:\markdown笔记\笔记图片\3-30.png)

  ![3-31](E:\markdown笔记\笔记图片\3-31.png)

  ![3-32](E:\markdown笔记\笔记图片\3-32.png)

## 卷积神经网络

###  卷积神经网络入门

* 神经网络遇到的问题

  * 参数过多

    举例：图像大小为1000*1000，下一层神经元个数为为10^6个，全连接参数为1000\*1000\*10^6=10^12各参数

    * 这样会造成过拟合，需要更多训练数据
    * 使得神经网络收敛到较差的局部极值

* 卷积

  图像数据具有非常强的区域性。我们可以对数据进行筛检，使全连接变为局部连接。如下图左图为全连接，右图为局部连接：

  ![3-33](E:\markdown笔记\笔记图片\3-33.png)

  * 局部连接利用了图像的区域性

    ![3-34](E:\markdown笔记\笔记图片\3-34.png)

  * 参数共享利用了图像特征与位置无关的性质

    让一组神经元使用相同的连接权

    ![3-35](E:\markdown笔记\笔记图片\3-35.png)

  * 卷积的计算

    其中**输出方阵的大小=输入方阵的大小-卷积核的大小+1**，请看下面的两幅图：

    ![3-36](E:\markdown笔记\笔记图片\3-36.png)

    ![3-37](E:\markdown笔记\笔记图片\3-37.png)

  * 计算方法

    ![3-38](E:\markdown笔记\笔记图片\3-38.gif)

    ![3-39](E:\markdown笔记\笔记图片\3-39.png)

  * 步长

     窗口一次滑动的长度。下图中滑动的步长为2的话，输出就是2x2了。在做卷积的时候默认步长是1。

    ![3-40](E:\markdown笔记\笔记图片\3-40.png)

  * 通过padding使得输出size不变

    我们通过卷积计算，输入的size就会减小，我们通过padding使得输出的size不变。我们通过**输出方阵的大小=输入方阵的大小-卷积核的大小+1**来计算padding的大小。

    ![3-42](E:\markdown笔记\笔记图片\3-42.png)

  * 卷积处理多通道

    对于输入样本中 `channels` 的含义。一般的RGB图片，`channels` 数量是 3 （红、绿、蓝）；

    一般 `channels` 的含义是，**每个卷积层中卷积核的数量**

    ![3-43](E:\markdown笔记\笔记图片\3-43.png)

  * 多个卷积核

    对于一个卷积核的物理含义来说卷积核上的参数都是一样的。卷积核是用来提取某种措施的，如果输入数据区域上有某种特征时，卷积核就能捕捉到这种特征，使得输出的值就比较大。如果输入数据区域上没有这个特征的时，输出的值就比较小。

    多个卷积核的物理含义说明我们可有从图像中提取多钟特征。

    ![3-45](E:\markdown笔记\笔记图片\3-45.png)

    ![3-44](E:\markdown笔记\笔记图片\3-44.png)

    上面这个问题：  有((3\*3)\*3）大小为一个卷积核，总共有((3*3)\*3)\*192=5184个卷积核

* 激活函数

  在卷积神经网络中常使用ReLU激活函数。

  以下激活函数的性质：

  * 单调性

  * 非线性函数：**为什么使用非线性函数作为激活函数？**

    * 如果使用线性激活函数或者不使用激活函数，那么无论你的神经网络有多少层，一直在做的只是计算线性激活函数，所以不如直接去掉全部隐藏层。

    * 假若网络中全部是线性部件，那么线性的组合还是线性，与单独一个线性分类器无异。这样就做不到用非线性来逼近任意函数。

    * 使用非线性激活函数 ，以便使网络更加强大，增加它的能力，使它可以学习复杂的事物，复杂的表单数据，以及表示输入输出之间非线性的复杂的任意函数映射。使用非线性激活函数，能够从输入输出之间生成非线性映射。

    

  ![3-46](E:\markdown笔记\笔记图片\3-46.png)

* 卷积总结

  * P=边距(padding)
  * S=步长(stride)
  * 通用公式：输出尺寸=(n-p)/s+1，其中n为输入大小，p为padding大小，s为步长
  * 参数数目：kw * kh * Ci * Co
    * Ci：输入通道数
    * Co：输出通道数
    * kw，kh：卷积核长宽

* 池化层

  * 最大池化

    在做卷积的时候默认步长是1，在做池化时步长大小与核的大小一致。在下图中，在窗口扫描时对于多出的数据列，我们可以忽略或者是增加padding的方法。其中最大池化操作的核里面是没有参数的，取窗口扫描的最大值，放到输出。

    ![3-47](E:\markdown笔记\笔记图片\3-47.png)

  * 平均值池化

    他就是将窗口扫描的数据求和平均，然后输出。

    ![3-38](E:\markdown笔记\笔记图片\3-38.png)

  * 池化的性质

    * 常使用不重叠，不补零。就是说窗口扫描的时候是不重叠的，多余的部分不添加padding，而是直接忽略
    * 没有用于求导的参数。操作的核里面是没有参数的，而是直接求最大值和平均值
    * 池化层参数为步长和池化核大小。一般我们将这两个大小设置为相同的
    * 池化的作用：用于减少图像尺寸，从而减少计算量
    * 一定程度解决平移鲁棒。我在图像上某个东西在某个位置，如果我们将这个东西在图像上偏移一点后，我们通过池化操作得到的激活值是一样的，也就是说偏移的影响被抹掉了
    * 损失了空间位置精度

* 全连接层（隐含层）

  * 将上一层输出展开并连接到每一个神经元上。全连接层的每一个结点都与上一层的所有结点相连，用来把前边提取到的特征综合起来。由于其全相连的特性，一般全连接层的参数也是最多的。全连接层将输出变成了一维的，后面不能再加卷基层和池化层了
  * 即普通神经网络的层
  * 相比于卷积层，参数的数目较大
  * 参数数目=Ci * Co
    * Ci/Co为输入输出通道数目

* 卷积神经网络结构

  * 卷积神经网络=卷基层+池化层+全连接层
  * 全卷积神经网络=卷积层+池化层，输入和输出是同等大小的图像

### 实战

#### 1.使用神经网络进行图像分类([neural_network](http://localhost:8888/notebooks/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/02-nn-basic/neural_network.ipynb))

#### 2.使用卷积神经网络进行图像分类([convnet](http://localhost:8888/notebooks/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/02-nn-basic/convnet.ipynb))

## 卷积神经网络进阶（模型演变）

* 模型演变

  AlexNet，VGG，ResNet，Inception，MobileNet

* 对比

* 为什么要讲不同的网络结构

  * 不同的网络结构解决的问题不同
  * 不同的网络结构使用的技巧不同
  * 不同的网络结构应用的场景不同

* 模型的进化

  * 更深更宽——AlexNet到VGGNet
  * 不同的模型结构——VGG到InceptionNet/ResNet
  * 优势组合——Inception+Res = InceptionResNet
  * 自我学习——NASNet
  * 实用——MobileNet

#### 1.AlexNet

* 介绍

  ![3-48](E:\markdown笔记\笔记图片\3-48.png)

  ![3-49](E:\markdown笔记\笔记图片\3-49.png)

  与普通卷积神经网络相同，包含有卷基层，池化层和全连接层，但他存在一个分割，将输入分给两个GPU去计算。

  ![3-50](E:\markdown笔记\笔记图片\3-50.png)

* 网络结构分析

  下面的这个公式要记住。

  ![3-51](E:\markdown笔记\笔记图片\3-51.png)

  ![3-53](E:\markdown笔记\笔记图片\3-53.png)

  * 首次实用Relu

    图中虚线是Sigmoid函数，实线是Relu函数

    ![3-52](E:\markdown笔记\笔记图片\3-52.png)

  * dropout

    ![3-54](E:\markdown笔记\笔记图片\3-54.jpg)

    上图右，在全连接后添加Dropout层的示意图。即在 **模 型 训 练 时** 随机让网络的某些节点不工作（输出置0），其它过程不变。

    ![3-54](E:\markdown笔记\笔记图片\3-54.png)

    ![3-55](E:\markdown笔记\笔记图片\3-55.png)

    ![3-56](E:\markdown笔记\笔记图片\3-56.png)

  * 其他细节

    ![3-58](E:\markdown笔记\笔记图片\3-58.png)

    第一点：将图像拓展为[256,256]，然后在此基础上用[224,224]的块去采集。

#### 2.VGGNet

![3-59](E:\markdown笔记\笔记图片\3-59.png)

* 网络结构

  ![3-61](E:\markdown笔记\笔记图片\3-61.png)

  ![3-62](E:\markdown笔记\笔记图片\3-62.png)

  其中这个28%的计算我来讲一下，参数数量的计算公式是这样的：**参数数量=输入通道数\*输出通道数\*输入size**。这里我们假设输入通道数与输出通道数相同都为a，那这个28%的计算就等于**(2\*3\*3\*a^2)/(5\*5\*a^2) = 0.28**。

  ![3-63](E:\markdown笔记\笔记图片\3-63.png)

  ![3-64](E:\markdown笔记\笔记图片\3-64.png)

  其中图中的画圈的A-LRN是做归一化。

* 训练技巧

  ![3-65](E:\markdown笔记\笔记图片\3-65.png)

#### 3.ResNet

![3-66](E:\markdown笔记\笔记图片\3-66.png)

![3-67](E:\markdown笔记\笔记图片\3-67.png)

![3-68](E:\markdown笔记\笔记图片\3-68.png)

![3-69](E:\markdown笔记\笔记图片\3-69.png)

![3-70](E:\markdown笔记\笔记图片\3-70.png)

在上图中x经过两个卷基层得到F(x)，我们对x做恒等变换，如果F(x)没有学到东西，那么我们让他等于0，忽略之前的两层网络。这就是上上幅图中所说的虽然增加了深度，但误差不会增加。

![3-71](E:\markdown笔记\笔记图片\3-71.png)

其中ResNet-34有34层网络。

![3-72](E:\markdown笔记\笔记图片\3-72.png)

![3-73](E:\markdown笔记\笔记图片\3-73.png)

![3-74](E:\markdown笔记\笔记图片\3-74.png)

#### 4.InceptionNet

![3-75](E:\markdown笔记\笔记图片\3-75.png)

稀疏矩阵的稀疏计算会被密集矩阵使用密集计算的方法更低效。
* V1结构
    ![3-76](E:\markdown笔记\笔记图片\3-76.png)

    卷基层还可以做拓展。

    ![3-77](E:\markdown笔记\笔记图片\3-77.png)

    ![3-79](E:\markdown笔记\笔记图片\3-79.png)

    参数和计算量比较：

    ![3-80](E:\markdown笔记\笔记图片\3-80.png)

    ![3-81](E:\markdown笔记\笔记图片\3-81.png)

    神经网络结构：

    ![3-82](E:\markdown笔记\笔记图片\3-82.png)

* V2结构

    用两个3x3的替换5x5的

    ![3-83](E:\markdown笔记\笔记图片\3-83.png)

* V3结构

    用一个3x1和一个1x3去替换3x3的，这里的3我们可以设为n，nxn的可以替换为1xn和nx1的，其中33%的计算过程是(3*3-(1\*3+3\*1))/3\*3=33%

    ![3-84](E:\markdown笔记\笔记图片\3-84.png)

* V4结构

  引入残差连接（skip connection）

  ![3-85](E:\markdown笔记\笔记图片\3-85.png)

  

    

    

    

    


















