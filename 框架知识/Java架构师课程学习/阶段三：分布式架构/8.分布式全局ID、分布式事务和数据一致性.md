# 8. 分布式全局ID、分布式事务和数据一致性

##  第一章 分布式全局ID

### 1-1 分布式全局id——概述和引发的问题

#### 1. 本章概述

1. 分库分表的系统中，由于id引发的问题
2. 使用UUID作为id实现主键全局唯一性保证
3. 通过统一ID序列表，实现全局id
4. 雪花算法作为全局id
5. 多种方案的比较

#### 3. 业界常见的分布式ID生成策略

分布式ID生成策略常见的有如下几种：

1. 数据库自增ID。
2. UUID生成。
3. Redis的原子自增方式。
4. 数据库水平拆分，设置初始值和相同的自增步长。
5. 批量申请自增ID。
6. 雪花算法。
7. 百度UidGenerator算法(基于雪花算法实现自定义时间戳)。
8. 美团Leaf算法(依赖于数据库，ZK)。
9. Twitter snowflake算法

#### 2. 分库分表引发的id问题

在通常情况下，每个表都有唯一标识，通常使用id，id通常采用自增的方式，在分库分表的情况下，每张表的id都是从0开始自增的。不同的分片上id可能是重复的，导致id在全局不唯一，进而出现业务上的混乱。

### 1-2 分布式主键UUID

#### 1. UUID

* UUID：通用唯一识别码（Universally Unique Identifier）
* 使用UUID，保证每一条记录的id都是不同的
* 缺点：只是单纯的一个id，没有实际意义，长度32位，太长了
* **Mycat不支持UUID的方式，Sharding-Jdbc支持UUID的方式**

#### 2. UUID实战

#### 前置准备

1. 将192.168.156.139和192.168.156.140的sharding-orders数据库中的orders_1和orders_2的order_id字段修改为`varchar(32)`
2. 将项目sharding-jdbc-demo的orders表的mapper文件重新生成

##### Spring Boot

1. 编写自定义的分片表达式

   ```java
   /**
    * @author : depers
    * @program : sharding-jdbc-demo
    * @description: 自定义UUID分片策略
    * @date : Created in 2021/8/30 21:28
    */
   public class UuidSharding implements PreciseShardingAlgorithm<String> {
       @Override
       public String doSharding(Collection<String> collection, PreciseShardingValue<String> preciseShardingValue) {
           String uuid = preciseShardingValue.getValue();
           int mode = uuid.hashCode() % collection.size();
   
           String[] strs = collection.toArray(new String[0]);
           mode = Math.abs(mode);
   
           System.out.println("分片表：" + strs[0] + ", " + strs[1]);
           System.out.println("mode = " + mode);
           return strs[mode];
       }
   }
   ```

2. 修改application.properties

   ```properties
   spring.shardingsphere.sharding.tables.orders.table-strategy.standard.sharding-column=order_id
   spring.shardingsphere.sharding.tables.orders.table-strategy.standard.precise-algorithm-class-name=cn.bravedawn.shardingjdbcdemo.sharding.UuidSharding
   
   spring.shardingsphere.sharding.tables.orders.key-generator.column=order_id
   spring.shardingsphere.sharding.tables.orders.key-generator.type=UUID
   ```

3. 运行测试程序

   ```java
   @Test
   public void insertByCustomOrderIdInsert(){
       Orders orders = new Orders();
       orders.setUserId(2);
       orders.setOrderAmount(BigDecimal.TEN);
       orders.setOrderStatus(0);
   
       ordersMapper.insertSelective(orders);
   }
   ```

4. 效果

   ![](../../../笔记图片/20/3-7/11.jpg)

##### Spring命名空间

1. 修改sharding-jdbc.xml

   ```xml
   <sharding:table-rules>
   	<sharding:table-rule logic-table="orders" actual-data-nodes="ms$->{0..1}.orders_$->{1..2}" database-strategy-ref="databaseStrategy" table-strategy-ref="standard" key-generator-ref="uuid"/>
   </sharding:table-rules>
   
   <sharding:key-generator id="uuid" column="order_id" type="UUID"/>
   
   <bean id="uuidSharding" class="cn.bravedawn.shardingjdbcdemo.sharding.UuidSharding"/>
   
   <sharding:standard-strategy id="standard" sharding-column="order_id" precise-algorithm-ref="uuidSharding"/>
   ```

2. 其他和SpringBoot相同

### 1-3 MyCat全局id （统一ID序列表）

* ID的值统一的从一个集中的ID序列生成器中获取。多个数据库从一个ID序列生成器中获取ID。

* **ID序列生成器MyCat支持，Sharding-Jdbc不支持。**

* MyCat中有两种方式：本地文件方式和数据库方式。

* 本地文件方式用于测试，数据库方式用于生产。
* 优点：ID集中管理，避免重复。
* 缺点：并发量大时，ID生成器压力较大。

#### 本地文件的方式

1. server.xml 中配置：

   ```xml
   <system><property name="sequnceHandlerType">0</property></system>
   ```


   注：sequnceHandlerType 需要配置为 0，表示使用本地文件方式。

2. sequence_conf.properties配置：

   ```
   MY_ORDER.HISIDS=
   MY_ORDER.MINID=1001
   MY_ORDER.MAXID=2000
   MY_ORDER.CURID=1000
   ```

   其中 `HISIDS` 表示使用过的历史分段(一般无特殊需要可不配置)，`MINID` 表示最小 ID 值，`MAXID` 表示最大
   ID 值，`CURID` 表示当前 ID 值。

3. schema.xml配置：

    ```xml
    <table name="my_order" dataNode="dn139,dn140" rule="auto-sharding-long" autoIncrement="true" primaryKey="id">
    </table>
    ```

4. 启动MyCat：`./mycat start`

5. 在mycat中执行该sql：

   ```sql
   INSERT INTO my_order ( id, total_amount, order_status )
   VALUES
   (
       next VALUE FOR mycatseq_my_order,
       88,
   	1
   )
   ```

   执行这条语句的时候会报错：

   ![](../../../笔记图片/20/3-7/12.jpg)

   解决办法是，修改schema.xml文件，将checkSQLschema置为false。

   ```xml
   <schema name="mycatdb" checkSQLschema="false" sqlMaxLimit="100">
   ```

   当该值设置为 true 时，如果我们执行语句(select * from USERDB.eg_user)则 MyCat 会把语句修改为(select * from eg_user)。即把表示 schema 的字符去掉，避免发送到后端数据库执行时报**（ERROR1146 (42S02): Table ‘ USERDB.eg_user’ doesn’ t exist）。 

   接下来执行这条sql就没有问题了。

6. 效果如下，可以看到id就是按照配置的顺序往下执行的

   ![](../../../笔记图片/20/3-7/13.jpg)

7. 如果我们执行如下SQL：

   ```
   INSERT INTO my_order (total_amount, order_status )
   VALUES
   (
       88,
   	1
   )
   ```

#### 数据库的方式

因为Mycat项目老是有一些迷之bug，放弃对该项目的学习。

### 1-4 分布式id——雪花算法

#### 1. 雪花算法

* SnowFlake是由Twitter提出的分布式ID算法
* 一个64bit的long型数字
* 引入了时间戳，保持递增

![](../../../笔记图片/20/3-8/雪花算法.jpg)

* 组成部分：
  1. **.第一位** 占用1bit，其值始终是0，没有实际作用，表示正数
  2. **时间戳** 占用41bit，精确到毫秒，总共可以容纳约69年的时间。
  3. **工作机器id** 占用10bit，其中高位5bit是数据中心ID，低位5bit是工作节点ID，做多可以容纳1024个节点。
  4. **序列号** 占用12bit，每个节点每毫秒0开始不断累加，最多可以累加到4095，一共可以产生4096个ID。
* SnowFlake算法在同一毫秒内最多可以生成多少个全局唯一ID呢：： **同一毫秒的ID数量 = 1024 x 4096 = 4194304**
* 基本保持全局唯一，同一机器在毫秒内并发最大可生成4096个ID
* 时间回调，可能会引起ID重复。就是服务器时间没有同步，需要将数据库时间修改为之前的时间。
* Mycat和Sharding-JDBC均支持雪花算法
* Sharding-JDBC可设置最大容忍回调时间

#### 2. Sharding-jdbc上的雪花算法实践

##### Spring Boot

1. 配置applications.proerties

   ```properties
   spring.shardingsphere.sharding.tables.orders.key-generator.column=order_id
   spring.shardingsphere.sharding.tables.orders.key-generator.type=SNOWFLAKE
   spring.shardingsphere.sharding.tables.orders.key-generator.props.worker.id=345
   spring.shardingsphere.sharding.tables.orders.key-generator.props.max.tolerate.time.difference.milliseconds=10
   ```

2. 运行测试程序：cn.bravedawn.shardingjdbcdemo.ShardingJdbcDemoApplicationTests#insertByCustomOrderIdInsert

##### Spring命名空间

1. 配置sharding-jdbc.xml

   ```xml
   <sharding:key-generator id="snowflake" column="order_id" type="SNOWFLAKE" props-ref="snow"/>
   
   <bean:properties id="snow">
       <!--worker.id：工作机器id，取值范围在0到1024
           max.tolerate.time.difference.milliseconds：最大容忍回调时间，单位ms
       -->
   	<prop key="worker.id">678</prop>
   	<prop key="max.tolerate.time.difference.milliseconds">10</prop>
   </bean:properties>
   
   <sharding:table-rule logic-table="orders" actual-data-nodes="ms$->{0..1}.orders_$->{1..2}" database-strategy-ref="databaseStrategy" table-strategy-ref="standard" key-generator-ref="snowflake"/>
   ```

2. 将139和140的sharding_orders数据库的orders表的id字段类型修改为bigint

3. 运行测试程序：cn.bravedawn.shardingjdbcdemo.ShardingJdbcDemoApplicationTests#insertByCustomOrderIdInsert

### 1-5 分布式全局ID方案落地

在mall项目中，我们使用的是idworker，github地址是：https://github.com/imadcn/idworker

## 第二章 分布式事务

### 2-1 概述

1. CAP原理
2. ACID原理和BASE原理
3. 基于XA协议的两阶段提交
4. 事务补偿机制
5. 基于本地消息表的最终一致性方案
6. 基于MQ消息队列的最终一致性方案

### 2-2 分布式系统中的CAP原理

![](../../../笔记图片/20/3-8/2.png)

### 2-3 由于分库分表引发的事务问题

* 传统的应用都是单一数据库事务，所有的业务都在同一数据库内，数据库的事务可以很好的得到支持。
* 在分布式系统中，将业务切分成多个数据库，多个独立的数据库之间，无法统一事务，造成数据不一致的情况。
  * 例如：将A用户的钱转给B用户，但是这两个用户的数据位于两个不同的数据库中，就可能会导致事务问题
  * 例如：将提交订单业务拆分为用户库、订单库、商品库。一个下单操作，用户使用积分购买商品，用户库扣减积分，订单库生成订单，商品库扣减库存。由于他们不在同一数据库，不能保证事务统一。
* 解决分布式事务的方案：
  1. 基于XA协议的两阶段提交
  2. 事务补偿机制
  3. 基于本地消息表+定时任务的最终一致性方案
  4. 基于MQ消息队列的最终一致性方案
* 参考博客：
  * https://segmentfault.com/a/1190000040321750
  * https://www.cnblogs.com/chengxy-nds/p/14046856.html

### 2-4 XA协议的两阶段提交（原理)

#### 1. 简介

* XA是由X/Open组织提出的分布式事务的规范

* XA规范主要定义了(全局)事务管理器(TM)和(局部)资源管理器(RM)之间的接口。

* 本地的数据库如mysql在XA中扮演的是RM角色

* 提交分为两个阶段：prepare和commit

#### 2.两阶段

* 第一阶段（prepare）：即所有的参与者RM准备执行事务并锁住需要的资源。参与者ready时，向TM报告已准备就绪。

  ![](../../../笔记图片/20/3-8/6.jpg)

* 第二阶段 (commit/rollback)：当事务管理者(TM)确认所有参与者(RM)都ready后，向所有参与者发送commit命令。

  ![](../../../笔记图片/20/3-8/7.jpg)

  目前主流的数据库基本都支持XA事务，包括mysql、oracle、sqlserver、postgre

#### 3. 特点

* 保证数据的强一致性
* *commit阶段出现问题，事务出现不一致，需人工处理*，**这句话我觉得有问题**
* 效率低下，性能与本地事务相差10倍

#### 4. 实现

1. MySQL5.7及以上均支持XA协议
2. MySQL Connector/J 5.0以上 支持XA协议
3. Java系统中实现中，数据源采用Atomikos

### 2-5 使用Atomikos做分布式事务

#### 1. 建表

将192.168.156.139和192.168.156.140的xa-139和xa-140数据库中的user表。

#### 2. 新建项目xa-demo，引入atomikos的依赖

* 项目地址：https://github.com/depers/mall/tree/master/xa-demo

* 引入atomikos依赖

  ```xml
  <dependency>
  	<groupId>org.springframework.boot</groupId>
  	<artifactId>spring-boot-starter-jta-atomikos</artifactId>
  </dependency>
  ```

#### 3. 多数据源配置

* 139数据源配置：cn.bravedawn.xademo.config.ConfigDB139
* 140数据源配置：cn.bravedawn.xademo.config.ConfigDB140
* XA事务管理器的声明：cn.bravedawn.xademo.config.ConfigDB139#jtaTransactionManager

#### 4. 测试

* 测试一：测试正常数据，两个数据库同时写入数据成功

  ```java
  @Transactional(transactionManager = "xaTransaction")
  public void testXa() throws InterruptedException {
      User user = new User();
      user.setUsername("张三");
      usersMapper139.insertSelective(user);
      usersMapper140.insertSelective(user);
  }
  ```

* 测试二：插入第一条数据之后，抛出异常，回滚之前的插入操作

  ```java
  @Transactional(transactionManager = "xaTransaction")
  public void testXa() throws InterruptedException {
      User user = new User();
      user.setUsername("张三");
      usersMapper139.insertSelective(user);
      // 除零来测试回滚，测试正常数据可以将这句测试语句删除
      int i = 1 / 0;
      usersMapper140.insertSelective(user);
  }
  ```

  ### 2-6 Sharding-JDBC的分布式事务

  按照课程的讲解，Sharding-JDBC好像无需配置就支持分布式事务。原生支持本地事务（弱事务）

  https://ericfu.me/sharding-jdbc-transaction-overview/
### 2-7 事务补偿机制原理

1. 什么是事务补偿机制：
     * 针对每个操作，都要注册一个与其对应的补偿（撤销）操作
     * 在执行失败时，调用补偿操作，撤销之前的操作
     
2. 例子：
     * A给B转账的例子，A和B在两家不同的银行
     * A账户减200元，B账户加200元
     * 两个操作要保证原子性，要么全成功，要么全失败
     * 由于A和B在两家不同的银行，所以存在部分不是事务的问题
     * 转账接口需要提供补偿机制
     * 如果A在扣减的过程中出现问题，直接抛出异常，事务回滚
     * B在增加余额的过程中，出现问题，要调用A的补偿接口
     * A之前的扣减操作，得到了补偿，进行了撤销
     * 保证了A和B的账是没有问题的
     
     具体流程如下图：
     
     ![](../../../笔记图片/20/3-8/3.jpg)
     
3. 优缺点
     * 优点：逻辑清晰，流程简单
     * 缺点：数据一致性比XA还要差，可能出错的点比较多
     * TCC属于应用层的一种补偿方式，程序员需要写大量代码

### 2-8 事务补偿机制程序示例

* 新建tcc-demo项目

  项目的详细代码请参考：

  其中值得一提的就是Spring 事务管理器的声明方式：

  ```java
  @Bean("tm139")
  public PlatformTransactionManager transactionManager(@Qualifier("db139") DataSource dataSource) {
      return new DataSourceTransactionManager(dataSource);
  }
  ```

* 测试事务补偿机制，这里涉及的数据库操作比较复杂，每一步出错都有可能导致两个数据库的数据出现不一致的情况。

  参考：cn.bravedawn.tccdemo.service.AccountService

### 2-9 本地消息表（原理）

#### 1. 概述

* 采用BASE原理，保证事务最终一致
* **在一致性方面，允许一段时间内的不一致，存在中间状态，但最终会一致。而对于XA/事务补偿机制来讲，他们是基于强一致性的，没有中间状态。**
* 在实际的系统当中，要根据具体的情况，判断是否采用。对数据的一致性特别高业务系统，就不建议采用。要求不高的业务，就可以考虑这种实现方式。

#### 2. 具体原理

下面我们描述一个微信支付的流程，其中支付服务和订单服务是两个数据库。

下面的这幅图中，右侧的数据库从上到下，第一个是支付服务的数据库，第二是订单服务的数据库。

左侧的从上到下，第一部分业务逻辑和定时任务是支付服务要做的事情，第二部分操作接口则是订单服务要做的事情。基于本地消息表的原理中：

1. 基于本地消息表的方案中，**业务逻辑**将**本事务外**的操作，记录到**消息表**当中；
2. 其他事务，提供操作接口，通知订单服务微信支付成功或是失败
3. **定时任务**轮询**本地消息表**，将未执行的消息发送给**操作接口**
4. **操作接口**处理成功，返回**成功**标识，处理失败返回**失败**标识
5. 定时任务接到标识，更新消息的状态
6. 定时任务按照一定的周期反复重试执行
7. 对于屡次失败的消息，可以设置最大失败次数
8. 超过最大失败次数的消息，不再进行接口调用，等待人工处理

![](../../../笔记图片/20/3-8/4.jpg)

#### 3. 优缺点

* 优点：避免了分布式事务，实现了最终一致性
* 缺点：要注意重试时的幂等性操作。例如在通知支付结果时，存在反复通知的情况，这里我们就要加幂等性控制。

### 2-10 本地消息表实战

#### 1. 数据库设计

在192.168.156.139的xa-139数据库中新建payment_msg表：

```sql
CREATE TABLE `payment_msg` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `order_id` int(11) NOT NULL,
  `status` int(1) NOT NULL DEFAULT 0 COMMENT '0：未发送，1：发送成功，2：超过最大发送次数',
  `failure_cnt` int(1) NOT NULL DEFAULT 0 COMMENT '失败次数，最大5次',
  `create_time` datetime NOT NULL,
  `create_user` int(11) NOT NULL,
  `update_time` datetime NOT NULL,
  `update_user` int(11) NOT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
```

在192.168.156.140的xa-140数据库中新建orders表：

```sql
CREATE TABLE `orders` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `order_status` int(1) NOT NULL COMMENT '0：未支付，1：已支付',
  `order_amount` decimal(10,2) NOT NULL,
  `receive_user` varchar(255) NOT NULL,
  `receive_mobile` varchar(11) NOT NULL,
  `create_user` int(11) NOT NULL,
  `create_time` datetime NOT NULL,
  `update_user` int(11) NOT NULL,
  `update_time` datetime NOT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
```

在项目xa-demo中生成这两张表的dao层、mapper和实体文件。

#### 2. 编写支付接口，消息落库

参考：cn.bravedawn.tccdemo.service.PaymentService#payment

#### 3. 编写订单操作接口

参考：cn.bravedawn.tccdemo.service.OrderService#handleOrder

#### 4. 定时任务

参考：cn.bravedawn.tccdemo.service.OrderScheduler#orderNotify

### 2-11 基于MQ消息队列的最终一致性方案

#### 1. 原理

* 原理、流程与本地消息表类似
* 不同点：
  * 将本地消息表改为了MQ
  * 定时任务改为MQ的消费者

![](../../../笔记图片/20/3-8/5.jpg)

#### 2. 优缺点

* 不依赖定时任务，基于MQ更高效、更可靠
* 适合于公司内的系统
* 不同公司之间交互无法基于MQ，本地消息表更合适

### 2-12 基于MQ的实现

这个实例我们仍然编写在tcc-demo项目中。

#### 1. RocketMQ的安装

* 下载RocketMQ，解压安装：`unzip rocketMQ.zip`

* 编辑runserver.sh和runbroker.sh修改JVM参数

* 编辑broker.properties，手动设置broker的ip地址。不然会报“RemotingConnectException: connect to <172.19.0.1:10909> failed”

  ```properties
  brokerIP1=192.168.156.135
  ```

* 分别运行如下命令启动rocketmq

  ```shell
  # Start Name Server
  nohup sh bin/mqnamesrv &
  
  # Start Broker
  nohup sh /bin/mqbroker -n localhost:9876 -c /usr/local/myapp/program/rocketmq-all-4.9.1-bin-release/conf/broker.properties &
  ```

* 关闭rocketmq

  ```shell
  # 关闭namesrv服务
  sh bin/mqshutdown namesrv
  
  # 关闭broker服务
  sh bin/mqshutdown broker
  ```

关于RocketMQ的使用可以参考博客：https://www.cnblogs.com/2YSP/p/11616376.html

#### 2. 生产者和消费者的定义

参考：cn.bravedawn.tccdemo.config.RocketMQConfig

#### 3. 编写生产者

参考：cn.bravedawn.tccdemo.service.PaymentService#paymentOfMQ

#### 4. 编写消费者

参考：cn.bravedawn.tccdemo.consumer.ChangeOrderStatus

