# 7.读写分离、分库分表

## 第一章 基础知识

### 1-1 海量数据的存储与访问瓶颈解决方案-数据切分

![](../../../笔记图片/20/3-7/1.png)

### 1-2 如何正确使用数据库读写分离

![](../../../笔记图片/20/3-7/2.png)

## 第二章 Mycat实战

### 2-1 MyCat概述与基本概念

![](../../../笔记图片/20/3-7/3.png)

### 2-2 分库分表概述

#### 1. 本章概述

* 互联网大数据时代，如何解决数据库性能瓶颈

* 读多写少的情况下，采用读写分离

* 海量数据的情况，采用数据切分

  * 数据切分方式一：垂直切分
  * 数据切分方式二：水平切分

* 实现读写分离和数据切分的两种模式

  * 模式一：中间层代理（例如：MyCat）

    ![](../../../笔记图片/20/3-7/4.jpg)

  * 模式二：客户端模式（例如：Sharding-Jdbc）

    ![](../../../笔记图片/20/3-7/5.jpg)

### 2-3  如何选择垂直切分、水平切分

#### 垂直切分

1. 切分规则：

   * 按照业务去切分
   * 每种业务一个数据库
   * 不同业务之间，禁止跨库join联查

2. 架构图

   ![](../../../笔记图片/20/3-7/6.jpg)

3. 优点

   * 拆分后业务清晰，拆分规则明确
   * 系统之间容易扩展和整合
   * 数据维护简单

4. 缺点

   * 部分业务表无法join，只能通过接口调用，提升系统的复杂度
   * 跨库事务难以处理
   * 垂直切分之后，某些业务数据过于庞大，仍然存在单体性能瓶颈

#### 水平切分

1. 切分规则

   * 将一张表的数据按照某种规则分到不同的数据库中

   * 需确定分片的规则

   * 使用分片字段查询时，可确定实体库。意思就是如果我是按照用户id进行的分片，这里的id就是分片字段，就可以确定去查那个库；

     使用其他字段查询时，要查询所有的表。意思是如果这个字段不是分片字段，我就要查所有的库，然后将他们的结果记性汇总返回；

2. 结构图

   ![](../../../笔记图片/20/3-7/7.jpg)

3. 优点

   * 解决了单库大数据、高并发的性能瓶颈
   * 拆分规则封装好，对应用端几乎透明，开发人员无需关系拆分细节
   * 提高了系统的稳定性和负载能力

4. 缺点

   * 拆分规则很难抽象
   * 分片事务一致性难以解决
   * 二次扩展时，数据迁移、维护难度大

#### 如何选择

先垂直后水平

### 2-4 MyCat-MySql安装

#### 1. MySQL的安装

关于MySQL的安装，我这里不再做赘述。在本案例中我们使用MariaDB代替

#### 2. MyCat的安装

1. 下载解压安装，这里我们使用的MyCat版本是1.6.7.3。切记一定要用这个版本，不然后面的流程会莫名报错。

2. 编辑conf/server.xml，配置MyCat用户

   ```xml
   <user name="root" defaultAccount="true">
   	<property name="password">123456</property>
   	<property name="schemas">user</property>
   	<property name="defaultSchema">user</property>
   </user>
   ```

3. 编辑schema.xml，这里我们使用的是`auto-sharding-long`算法进行分片

  ```xml
  <?xml version="1.0"?>
  <!DOCTYPE mycat:schema SYSTEM "schema.dtd">
  <mycat:schema xmlns:mycat="http://io.mycat/">
  
  	<schema name="u" checkSQLschema="true" sqlMaxLimit="100" randomDataNode="dn139">
  		<table name="user" primaryKey="id" dataNode="dn139,dn140" rule="sharding-by-intfile" autoIncrement="true" fetchStoreNodeByJdbc="true">
  		</table>
  	</schema>
  
  	<dataNode name="dn139" dataHost="db139" database="user_139" />
  	<dataNode name="dn140" dataHost="db140" database="user_140" />
  
  	<dataHost name="db139" maxCon="1000" minCon="10" balance="0"
  			  writeType="0" dbType="mysql" dbDriver="jdbc" switchType="1"  slaveThreshold="100">
  		<heartbeat>select user()</heartbeat>
  		<!-- can have multi write hosts -->
  		<writeHost host="M1" url="jdbc:mysql://192.168.156.139:3306" user="root"
  				   password="fx1212">
  		</writeHost>
  	</dataHost>
  
  	<dataHost name="db140" maxCon="1000" minCon="10" balance="0"
  			  writeType="0" dbType="mysql" dbDriver="jdbc" switchType="1"  slaveThreshold="100">
  		<heartbeat>select user()</heartbeat>
  		<!-- can have multi write hosts -->
  		<writeHost host="M1" url="jdbc:mysql://192.168.156.140:3306" user="root"
  				   password="fx1212">
  		</writeHost>
  	</dataHost>
  
  </mycat:schema>
  ```
  
4. 查看rule.xml

   ```xml
   <tableRule name="auto-sharding-long">
       <rule>
           <columns>id</columns>
           <algorithm>rang-long</algorithm>
       </rule>
   </tableRule>
   ```

   接着我们看下`rang-long`算法

   ```xml
   <function name="rang-long"
       class="io.mycat.route.function.AutoPartitionByLong">
       <property name="mapFile">autopartition-long.txt</property>
   </function>
   ```

   接着我们去看下`autopartition-long.txt`文件

   ```txt
   # range start-end ,data node index
   # K=1000,M=10000.
   0-500M=0
   500M-1000M=1
   1000M-1500M=2
   ```

   其中这里配置了三个分片的id区间的规则，但是我们只配置了两台数据库服务器，所以这里我们需要修改下逻辑：

   ```
   # range start-end ,data node index
   # K=1000,M=10000.
   0-500M=0
   500M-1000M=1
   ```

5. 启动MyCat的时候，报错

   ```
   [root@es1 mycat]# ./bin/mycat console
   Running Mycat-server...
   wrapper  | --> Wrapper Started as Console
   wrapper  | Launching a JVM...
   wrapper  | Startup failed: Timed out waiting for a signal from the JVM.
   wrapper  | JVM did not exit on request, terminated
   wrapper  | JVM exited on its own while waiting to kill the application.
   wrapper  | JVM exited in response to signal SIGKILL (9).
   wrapper  | Launching a JVM...
   wrapper  | INT trapped.  Shutting down.
   wrapper  | JVM exited unexpectedly while stopping the application.
   wrapper  | <-- Wrapper Stopped
   ```

   解决办法：编辑/conf/wrapper.conf文件，配置超时策略

   ```
   wrapper.startup.timeout=300 //超时时间300秒
   wrapper.ping.timeout=120
   ```

   重新启动：`./mycat console`

6. 通过Navicat连接的时候会出现几个问题

   1. 切记连接的时候使用 **8066**端口
   2. [**navicate连接mycat报1184错误**](http://www.hu-rong.com/article/499s)

7. 测试一：在MyCat中插入id=1，username=fengxiao的数据，观察192.168.156.139的user表中的数据，你会发现这条数据实际上被插入到了这个库中

8. 测试二：在MyCat中插入id=5000001，username=fengxiao2的数据，观察192.168.156.140的user表中的数据，你会发现这条数据实际上被插入到了这个库中。这也恰好验证了之前autopartition-long.txt文件的分片配置

### 2-5. Mycat中的分库分表、读写分离的配置

#### 1. server.xml配置

* 配置MyCat的用户名、密码、权限、Schema等。

  ```xml
  <user name="root" defaultAccount="true">
      <property name="password">123456</property>
      <property name="schemas">user,order,product</property>
      <!--这里的schema和schema中的schema标签声明相对应-->
      <property name="defaultSchema">user,order,product</property>	
  </user>
  
  <user name="user">
      <property name="password">user</property>
      <property name="schemas">TESTDB</property>
      <property name="readOnly">true</property>
      <property name="defaultSchema">TESTDB</property>
  </user>
  ```

* 如同给MySql新建用户一样
* 客户端连接MyCat与连接MySql无异

#### 2. MySql8兼容

在MySql8由于修改了原有的加密方式，如果仍要按照之前的加密方式登录mysql命令行工具的话，操作命令如下：

```
mysql -uroot -p -P8066 -h127.0.0.1 --default-auth=mysql_native_password
```

### 2-6. schema.xml基础标签简介

* 配置dataHost（节点主机），包括读host、写host

  ```xml
  <dataHost name="localhost1" maxCon="1000" minCon="10" balance="0"
  		  writeType="0" dbType="mysql" dbDriver="jdbc" switchType="1"  slaveThreshold="100">
  	<!--balance 针对读请求的负载均衡类型：
  		0不开启读写分离，也就是说所有的读请求落在writeHost上；
  		1和2读写均匀分配，如果我在写库里面配置了多个读库，那么读请求既有可能落到读库上也有可能落在写库上；
  		3读落在readHost上，所有的读请求都会落到读请求上；-->
  	<!--writeType 针对所有的写请求(insert/update/delete)：
  		0表示你所有的写请求会落在第一个writeHost上面，建议使用0；
  		1表示随机；-->
  	<heartbeat>select user()</heartbeat>
  	<!-- can have multi write hosts -->
  	<writeHost host="hostM1" url="jdbc:mysql://localhost:3306" user="root"
  			   password="root">
  			<!-- can have multi read hosts-->
  	</writeHost>
  	<!-- <writeHost host="hostM2" url="localhost:3316" user="root" password="123456"/> -->
  </dataHost>
  ```

* 配置dataNode（数据节点），指定到具体的数据库

  ```xml
  <dataNode name="dn1" dataHost="localhost1" database="db1" />
  ```

* 配置schema，表名，数据节点，分片规则等

  ```xml
  <!--checkSQLschema: 是否去掉SQL中的数据库名，select * from user139.user; 去除user139;
      sqlMaxLimit: 指的就是检索数据的时候的默认会给一个limit，仅对分片表有效;
  	randomDataNode：属性定义将一些随机语句发送到该数据节点中
   -->
  <schema name="TESTDB" checkSQLschema="true" sqlMaxLimit="100" randomDataNode="dn1"</schema>
  ```


### 2-7. schema配置实操

* Mycat的第二种启动方式：`mycat start`

* 修改了Mycat的schema.xml文件，如何实现不重启mycat使得配置生效呢？
  * 修改了schema.xml文件
  * Mycat有一个管理端的连接：**端口9066**
  * 使用navicat连接之后，选择数据库，接着右键单击选择命令列界面，进行如下操作：
    * `show @@help`;
    * `reload @@config`，通过执行这条命令就可以使刚才修改的schema.xml文件生效
    * `reload @@config_all`，如果你修改了`dataHost`节点的内容，要想生效得执行这条命令

### 2-8. MySQL主从搭建

这里我们搭建主从数据库的服务器使用的是：192.168.156.139（主），192.168.156.138（从）

* MySQL主从

  * 主配置log-bin，指定文件的名字。编辑MySQL主上的/etc/my.cnf的[mysqld]：

    ```
    log-bin=imooc_mysql
    ```

  * 主配置server-id，默认为1。MySQL实例中全局唯一，并且大于0。编辑MySQL主上的/etc/my.cnf的[mysqld]：

    ```
    server-id=1
    ```

  * 从配置server-id，与主不能重复。MySQL实例中全局唯一，并且大于0。与主上的 server-id 区分开。编辑MySQL从上的/etc/my.cnf的[mysqld]：

    ```
    server-id=2
    ```

  * 主库创建备份账户并授权 `REPLICATION SLAVE`，具体的操作命令如下（MySQL shell）：

    ```sql
    # 创建用户，其中%指任意ip
    create user 'repl'@'%' identified by 'immoc@1234';
    # 授权
    grant replication slave on *.* to 'repl'@'%';
    # 刷新
    flush privileges;
    ```

  * 主库进行锁表，阻止所有的写入操作（MySQL shell）： `FLUSH TABLES WITH READ LOCK;`

  * 主库找到log-bin的位置 （MySQL shell）：`SHOW MASTER STATUS;`

  * 将主库的数据进行备份（Linux shell）：`mysqldump --all-databases --master-data > dbdump.db -uroot -pfx1212`，上面这句命令得去复制一个新的linux shell中去执行。**切记这里不要退出上面执行命令的mysql shell，否则上面的锁表操作就会失效，数据就会入库。**

  * 在从库的服务器上复制主库服务器上的dump文件：`scp root@192.168.156.139:~/dbdump.db .`。接着将dump的数据导入140的这台MySQL中：`mysql < dbdump.db -uroot -pfx1212`

  * 主库进行解锁，解锁后，主上可以写入数据（MySQL shell）：`unlock tables`

  * 从库设置主从配置（MySQL shell）：

    ```sql
    CHANGE MASTER TO
    MASTER_HOST='master_host_name',
    MASTER_USER='replication_user_name',
    MASTER_PASSWORD='replication_password',
    MASTER_LOG_FILE='recorded_log_file_name',
    MASTER_LOG_POS='recorded_log_position';
    
    # 具体的配置
    change master to 
    master_host='192.168.156.139',
    master_user='repl',
    master_password='immoc@1234',
    master_log_file='imooc_mysql.000001',
    master_log_pos=330;
    ```

    然后执行：`start slave`

  * 从库查看MySQL从的状态：`show slave status`

### 2-9. 分片规则：枚举分片算法

  * schema.xml配置

    * rule属性：定义分片表的分片规则，必须与`rule.xml`中的tableRule对应

  * 分片枚举

    * 使用：在配置文件中配置可能的枚举id，自己配置分片节点。主要的使用场景就是对**可枚举的值**进行单独的分片配置。

    * schema.xml全量配置，将table标签的rule属性修改为`sharding-by-intfile`：

      ```xml
      <?xml version="1.0"?>
      <!DOCTYPE mycat:schema SYSTEM "schema.dtd">
      <mycat:schema xmlns:mycat="http://io.mycat/">
      
      	<schema name="user" checkSQLschema="true" sqlMaxLimit="100" randomDataNode="dn139">
      		<table name="user" primaryKey="id" dataNode="dn139,dn140" rule="sharding-by-intfile" autoIncrement="true" fetchStoreNodeByJdbc="true">
      		</table>
      	</schema>
      
      	<dataNode name="dn139" dataHost="db139" database="user_139" />
      	<dataNode name="dn140" dataHost="db140" database="user_140" />
      
      	<dataHost name="db139" maxCon="1000" minCon="10" balance="0"
      			  writeType="0" dbType="mysql" dbDriver="jdbc" switchType="1"  slaveThreshold="100">
      		<heartbeat>select user()</heartbeat>
      		<!-- can have multi write hosts -->
      		<writeHost host="M1" url="jdbc:mysql://192.168.156.139:3306" user="root"
      				   password="fx1212">
      			<readHost host="S1" usl="192.168.156.138:3306" user="root" password="fx1212">
      			</readHost>
      		</writeHost>
      	</dataHost>
      
      	<dataHost name="db140" maxCon="1000" minCon="10" balance="0"
      			  writeType="0" dbType="mysql" dbDriver="jdbc" switchType="1"  slaveThreshold="100">
      		<heartbeat>select user()</heartbeat>
      		<!-- can have multi write hosts -->
      		<writeHost host="M1" url="jdbc:mysql://192.168.156.140:3306" user="root"
      				   password="fx1212">
      		</writeHost>
      	</dataHost>
      
      </mycat:schema>
      ```

    * rule.xml配置，修改枚举分片的特定列，这里我们改成`province_id`。除此之外，配置`<property name="defaultNode">0</property>`，设置默认节点。

      ```xml
      <tableRule name="sharding-by-intfile">
          <rule>
          	<columns>province_id</columns>
          	<algorithm>hash-int</algorithm>
          </rule>
      </tableRule>
      ...
      <function name="hash-int"
                class="io.mycat.route.function.PartitionByFileMap">
          <property name="mapFile">partition-hash-int.txt</property>
          <property name="defaultNode">0</property>
      </function>
      ```

    *  修改partition-hash-int.txt，设置枚举规则

      ```
      10000=0
      10010=1
      DEFAULT_NODE=0
      ```

    * 达到的效果就是，province_id=10000的数据将会保存到139节点上，province_id=10010的数据将会保存到140节点上。

### 2-10 分片规则：取模

1. 修改schema.xml，将table标签的rule属性修改为`mod-long`

2. 修改rule.xml

   ```xml
   <tableRule name="mod-long">
       <rule>
       	<columns>user_id</columns>
       	<algorithm>mod-long</algorithm>
       </rule>
   </tableRule>
   
   <function name="mod-long" class="io.mycat.route.function.PartitionByMod">
   	<property name="count">3</property>
   </function>
   ```

3. reload配置：`reload @@config_all`

### 2-11 全局表

假如我们有这么一种场景，省市区的表数据基本不不会变，如果我们也采用分表的话，在不同的分片（不同的数据库）上做联表查询是是非常耗时的。为了解决这个问题，我们可以把这张表设置为全局表，这样的话每个分片上都放一份这样的数据，做联表查询的时候只会在自己分片的表上做联表查询。

1. 在139和140的数据库中，新建省份表：

   ```sql
   CREATE TABLE `province` (
     `province_id` int(11) NOT NULL,
     `province_name` varchar(255) DEFAULT NULL,
     PRIMARY KEY (`province_id`)
   ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
   ```

2. 修改schema.xml，修改如下：

   ```xml
   <schema name="user" checkSQLschema="true" sqlMaxLimit="100" randomDataNode="dn139">
   	<table name="user" primaryKey="id" dataNode="dn139,dn140" rule="sharding-by-intfile" autoIncrement="true" fetchStoreNodeByJdbc="true"/>
       <!--添加global表配置-->
   	<table name='province' dataNode="dn139,dn140" type="global"/>
   </schema>
   ```

3. reload配置：`reload @@config_all`

### 2-12 子表

1. 背景：拿订单表来举例，有一张order表，还有一张order_item表。如果我们对order表进行了水平切分，那这张表中的数据就会按照一定的分片规则，分散到不同的数据库中。我们经常会对order和order_item表进行联表查询，若对order_item表也做了分片，往往会将有关联关系的order_item数据分配到与对应order数据不相同的数据库上。为了解决这个问题，我们使用子表。他会将order和order_item的有关联的数据分配到同一分片上。

2. schema.xml配置
   * childTable标签：定义分片子表
   * nane属性：子表名称
   * joinKey属性：标志子表中的列，用于与父表做关联
   * parentKey属性：标志父表中的列，与joinKey对应
   
3. 实践

   * 编辑schema文件

     ```xml
     <schema name="mycatdb" checkSQLschema="true" sqlMaxLimit="100">
     	<table name="user" primaryKey="id" dataNode="dn139,dn140" rule="sharding-by-intfile" autoIncrement="true"/>
     	<table name='province' dataNode="dn139,dn140" type="global"/>
     	<!-- 添加子表配置 -->
     	<table name="my_order" dataNode="dn139,dn140" rule="auto-sharding-long">
     		<childTable name="order_item" joinKey="order_id" parentKey="id"/>
     </table>
     </schema>
     ```

   * 新建my_order表和order_item表

     ```sql
     CREATE TABLE `my_order` (
       `id` int(11) NOT NULL,
       `total_amount` decimal(10,2) NOT NULL,
       `order_status` tinyint(1) NOT NULL,
       PRIMARY KEY (`id`)
     ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
     ```

     ```sql
     CREATE TABLE `order_item` (
       `id` int(11) NOT NULL,
       `order_id` int(11) NOT NULL,
       `product_name` varchar(255) NOT NULL,
       `num` int(11) NOT NULL,
       PRIMARY KEY (`id`)
     ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
     ```

   * 插入两组数据

     ```sql
     insert into my_order (id, total_amount, order_status) values(1, 10,1);
     insert into my_order (id, total_amount, order_status) values(6000000, 100,1);
     
     insert into order_item (id, order_id, product_name, num) values(1, 1, '测试商品', 1);
     insert into order_item (id, order_id, product_name, num) values(2, 6000000, '测试商品2', 2);
     ```

   * 得到的效果就是：

     在139数据库中的my_order表中只有id为1的order和id为1的order_item数据；

     在140数据库中的my_order表中只有id为6000000的order和id为2的order_item数据；

### 2-13 Mycat的高可用原理

* 目的：避免Mycat成为系统中的单点

* Mycat的高可用架构：

  ![](../../../笔记图片/20/3-7/8.jpg)
  
  具体上面的这幅图可以参考博客：https://blog.csdn.net/wdw1206/article/details/44201331
  
* 具体实践

  1. 分别在192.168.156.138和192.168.156.139上面安装Mycat，可以直接将138的mycat复制过去，命令：`scp -r root@192.168.156.138:/usr/local/myapp/program/mycat .`

  2. 分别在192.168.156.138和192.168.156.140上安装haproxy

     * 执行：`yum search haproxy`
     * 安装：`yum -y install haproxy.x86_64 `

  3. 编辑配置文件：`vi /etc/haproxy/haproxy.cfg`

     ```
     # 连接haproxy的端口
     frontend  main *:5000
     
     # 采用tcp通信
     defaults
         mode                    tcp
         
     # 配置循环法，负载到两台mycat上
     backend app
         balance     roundrobin
         server  app1 192.168.156.138:8066 check
         server  app2 192.168.156.139:8066 check
     ```

  4. 分别启动haproxy：`haproxy -f /etc/haproxy/haproxy.cfg`

  5. 【问题版本】分别在192.168.156.138和192.168.156.140上安装keepalived，具体安装步骤参考之前的章节。这里我们把keepalived的核心配置文件给出：

     192.168.156.138配置：

     ```shell
     ! Configuration File for keepalived
     
     global_defs {
        # 路由id：当前安装keepalived节点主机的标识符，全局唯一
        router_id keep_140
     }
     
     
     # 基于vrrp协议的一个实例节点
     vrrp_instance VI_1 {
         # 表示节点的状态，当前的136为Nginx的主节点，这里有两个可配置参数 MASTER/BACKUP
         state MASTER
         # 当前实例绑定的网卡
         interface ens33
         # 虚拟的路由id，主备节点配置一致即可
         virtual_router_id 51
         # 优先级/权重，谁的优先级高，在MASTER挂掉以后，就能成为MASTER
         priority 80
         # 主备之间同步检查的时间间隔，默认为1s
         advert_int 1
         # 认证授权的密码，防止非法节点的进入
         authentication {
             auth_type PASS
             auth_pass 1111
         }
         # 虚拟IP配置
         virtual_ipaddress {
             192.168.156.161
         }
     }
     
     # 标识一个虚拟服务器定义块
     virtual_server 192.168.156.161 6000 {
         # 以秒为单位指定检查间隔，这里设置为6秒
         delay_loop 6
         # 配置负载均衡选择特定的调度程序，rr-轮询
         lb_algo rr
         # 选择特定的转发方法
         lb_kind NAT
         # 指定持久连接的超时值
         persistence_timeout 50
         # 指定协议类型
         protocol TCP
     
         # 指定真实的服务器成员
         real_server 192.168.156.138 5000 {
             # 指定负载均衡决策的真实服务器权重
             weight 1
             # 使用TCP Connect检查真实服务器可用性,是keepalived在做负载均衡的时候，会去检测这个真实节点的可用性，如果不可以将会剔除该节点。在这里这个real_server可以配置多个
             TCP_CHECK {
                 connect_port 5000
                 connect_timeout 10000
             }
         }
     
     }
     ```

     192.168.156.140配置：

     ```shell
     ! Configuration File for keepalived
     
     global_defs {
        # 路由id：当前安装keepalived节点主机的标识符，全局唯一
        router_id keep_140
     }
     
     
     # 基于vrrp协议的一个实例节点
     vrrp_instance VI_1 {
         # 表示节点的状态，当前的136为Nginx的主节点，这里有两个可配置参数 MASTER/BACKUP
         state BACKUP
         # 当前实例绑定的网卡
         interface ens33
         # 虚拟的路由id，主备节点配置一致即可
         virtual_router_id 51
         # 优先级/权重，谁的优先级高，在MASTER挂掉以后，就能成为MASTER
         priority 80
         # 主备之间同步检查的时间间隔，默认为1s
         advert_int 1
         # 认证授权的密码，防止非法节点的进入
         authentication {
             auth_type PASS
             auth_pass 1111
         }
         # 虚拟IP配置
         virtual_ipaddress {
             192.168.156.161
         }
     }
     
     # 标识一个虚拟服务器定义块
     virtual_server 192.168.156.161 6000 {
         # 以秒为单位指定检查间隔，这里设置为6秒
         delay_loop 6
         # 配置负载均衡选择特定的调度程序，rr-轮询
         lb_algo rr
         # 选择特定的转发方法
         lb_kind NAT
         # 指定持久连接的超时值
         persistence_timeout 50
         # 指定协议类型
         protocol TCP
     
         # 指定真实的服务器成员
         real_server 192.168.156.140 5000 {
             # 指定负载均衡决策的真实服务器权重
             weight 1
             # 使用TCP Connect检查真实服务器可用性,是keepalived在做负载均衡的时候，会去检测这个真实节点的可用性，如果不可以将会剔除该节点。在这里这个real_server可以配置多个
             TCP_CHECK {
                 connect_port 5000
                 connect_timeout 10000
             }
         }
     
     }
     ```

     接着启动keepalived：`./sbin/keepalived -f /etc/keepalived/keepalived.conf`

     如下图就说明启动成功了：

     ![](../../../笔记图片/20/3-7/9.jpg)

     上面的脚本也有问题，他只是实现了keepalived之间的主备切换，若是其中一台haproxy宕机，则不能完成keepaliaved的切换，进而导致整个服务不可用。

  6. 【完善版】去除TCP_CHECK，增加track_script监测脚本：

     192.168.156.138配置：

     ```shell
     ! Configuration File for keepalived
     
     global_defs {
        # 路由id：当前安装keepalived节点主机的标识符，全局唯一
        router_id keep_140
     }
     
     vrrp_script check_haproxy_alive {
         # 若该命令返回0，则说明haproxy正常运行，返回1则说明haproxy挂了，进而触发keepalived进行主备切换
         script "killall -0 haproxy"
         interval 2 # 每隔2秒运行上一行脚本
     }
     
     
     # 基于vrrp协议的一个实例节点
     vrrp_instance VI_1 {
         # 表示节点的状态，当前的136为Nginx的主节点，这里有两个可配置参数 MASTER/BACKUP
         state MASTER
         # 当前实例绑定的网卡
         interface ens33
         # 虚拟的路由id，主备节点配置一致即可
         virtual_router_id 51
         # 优先级/权重，谁的优先级高，在MASTER挂掉以后，就能成为MASTER
         priority 80
         # 主备之间同步检查的时间间隔，默认为1s
         advert_int 1
         # 认证授权的密码，防止非法节点的进入
         authentication {
             auth_type PASS
             auth_pass 1111
         }
         # 虚拟IP配置
         virtual_ipaddress {
             192.168.156.161
         }
     
         # 配置监测haproxy的脚本
         track_script {
             check_haproxy_alive
         }
     }
     
     # 标识一个虚拟服务器定义块
     virtual_server 192.168.156.161 6000 {
         # 以秒为单位指定检查间隔，这里设置为6秒
         delay_loop 6
         # 选择特定的调度程序，rr-轮询
         lb_algo rr
         # 选择特定的转发方法
         lb_kind NAT
         # 指定持久连接的超时值
         persistence_timeout 50
         # 指定协议类型
         protocol TCP
     
         # 指定真实的服务器成员
         real_server 192.168.156.138 5000 {
             # 指定负载均衡决策的真实服务器权重
             weight 1
         }
     
     }
     ```

     192.168.156.140配置：

     ```shell
     ! Configuration File for keepalived
     
     global_defs {
        # 路由id：当前安装keepalived节点主机的标识符，全局唯一
        router_id keep_140
     }
     
     vrrp_script check_haproxy_alive {
         # 若该命令返回0，则说明haproxy正常运行，返回1则说明haproxy挂了，进而触发keepalived进行主备切换
         script "killall -0 haproxy"
         interval 2 # 每隔2秒运行上一行脚本
     }
     
     
     # 基于vrrp协议的一个实例节点
     vrrp_instance VI_1 {
         # 表示节点的状态，当前的136为Nginx的主节点，这里有两个可配置参数 MASTER/BACKUP
         state BACKUP
         # 当前实例绑定的网卡
         interface ens33
         # 虚拟的路由id，主备节点配置一致即可
         virtual_router_id 51
         # 优先级/权重，谁的优先级高，在MASTER挂掉以后，就能成为MASTER
         priority 80
         # 主备之间同步检查的时间间隔，默认为1s
         advert_int 1
         # 认证授权的密码，防止非法节点的进入
         authentication {
             auth_type PASS
             auth_pass 1111
         }
         # 虚拟IP配置
         virtual_ipaddress {
             192.168.156.161
         }
     
         # 配置监测haproxy的脚本
         track_script {
             check_haproxy_alive
         }
     }
     
     # 标识一个虚拟服务器定义块
     virtual_server 192.168.156.161 6000 {
         # 以秒为单位指定检查间隔，这里设置为6秒
         delay_loop 6
         # 选择特定的调度程序，rr-轮询
         lb_algo rr
         # 选择特定的转发方法
         lb_kind NAT
         # 指定持久连接的超时值
         persistence_timeout 50
         # 指定协议类型
         protocol TCP
     
         # 指定真实的服务器成员
         real_server 192.168.156.140 5000 {
             # 指定负载均衡决策的真实服务器权重
             weight 1
         }
     
     }
     ```

     以上配置就可以实现haproxy宕机后，keepalived会主动切换主备的逻辑，实现了真正的高可用。

  

